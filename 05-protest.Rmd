# Deployments and Protest Activity

```{r setup , echo = FALSE, include = FALSE}

#devtools::install_github("lindeloev/job")

library(tidyverse)
library(tidyr)
library(job)
library(broom)
library(broom.mixed)
library(psych)
#library(ipw)
library(modelr)
library(cshapes)
library(countrycode)
library(dagitty)
library(ggdag)
library(arm)
library(brms)
library(rstan)
library(cmdstanr)
library(parallel)
library(tidybayes)
library(scales)
library(ggdist)
library(kableExtra)
library(BayesPostEst)
library(texreg)
library(modelsummary)
library(gt)
library(webshot)
library(kableExtra)
library(ggExtra)
library(ggmcmc)
library(ggpubr)
library(patchwork)
library(here)
library(arm)
library(performance)
#library(ROCR)
library(tictoc)
library(remotes)

#devtools::install_github('tidyss/rroughviz')
#devtools::install_github("xvrdm/ggrough")
#library(ggrough)
#library(rroughviz)

#remotes::install_github('vincentarelbundock/modelsummary') 

# Test
library(modelsummary)

knitr::opts_chunk$set(comment = '', dpi = 400, fig.align = "center", out.width = "75%")

sysfonts::font_add_google("Oswald", family = "oswald")
showtext::showtext_auto()

# Set basesize for fonts in plots
basesize <- 11


# Regular plot theme
theme_flynn <- function(){ 
  
      theme_linedraw(base_size = basesize, base_family = "oswald") %+replace% 
        
        theme(plot.title = element_text(face = "bold", size = basesize * 1.3, hjust = 0, margin = margin(t = 0, b = 0.3, l = 0, r = 0, unit = "cm")),
              plot.subtitle = element_text(size = basesize),
              plot.caption = element_text(face = "italic", size = basesize * 0.6),
              panel.border = element_rect(fill = NA, size = 0.2),
              strip.background = element_rect(fill = "gray80", color = "black", size = 0.2),
              strip.text = element_text(size = basesize, color = "black", face = "bold", margin = margin(t = 0.2, b = 0.2, l = 0.2, r = 0.2, unit = "cm")),
              panel.background = element_rect(size = 0.2),
              panel.grid.major = element_line(color = "gray70", size = 0.15),
              panel.grid.minor = element_line(color = "gray90", size = 0.1),
              axis.title = element_text(face = "bold", size = basesize),
              axis.title.y = element_text(angle = 90, margin = margin(t = 0, r = 0.5, b = 0, l = 0, unit = "cm")),
              axis.title.x = element_text(margin = margin(t = 0.5, r = 0, b = 0, l = 0, unit = "cm")),
              axis.ticks = element_line(size = 0.1),
              axis.ticks.length = unit(0.1, "cm"),
              legend.title = element_text(size = basesize, face = "bold", hjust = 0, margin = margin(t = 0, b = 0, l = 0, r = 0, unit = "cm")),
              plot.margin = margin(0.5, 0.5, 0.5, 0.5, unit = "cm"),
              legend.margin = margin(t=-10, b=0, r=0, l=0),
              legend.box.margin = margin(-10,-10,-10,-10))
  }

# Map version of theme
theme_flynn_map <- function(){
  
  theme_void(base_family = "oswald") %+replace% 
    
    theme(plot.title = element_text(face = "bold", size = 18, hjust = 0, margin = margin(t = 0, b = 0.3, l = 0, r = 0, unit = "cm")),
          plot.subtitle = element_text(size = 12, hjust = 0, margin = margin(t = 0, b = 0.3, l = 0, r = 0, unit = "cm")),
          plot.caption = element_text(face = "italic", size = 8, hjust = 1, margin = margin(t = 0.2, unit = "cm")),
          strip.background = element_rect(fill = "gray80", color = "black"),
          strip.text = element_text(color = "black", face = "bold"),
          panel.grid.major = element_line(color = "white", size = 0),
          panel.grid.minor = element_line(color = "white", size = 0),
          #axis.title = element_text(face = "bold", size = 0),
          #axis.title.y = element_text(margin = margin(t = 0, r = 0.5, b = 0, l = 0, unit = "cm")),
          #axis.title.x = element_text(margin = margin(t = 0.5, r = 0, b = 0, l = 0, unit = "cm")),
          legend.title = element_text(face = "bold"),
          legend.position = "bottom",
          legend.key.height = unit(0.6, "cm"),
          legend.key.width = unit(2.5, "cm"))
}


p.data <- read_csv(here::here("../Book/Data/Chapter-Protests/protest-data-final.csv")) %>% 
  group_by(ccode) %>% 
  arrange(ccode, year) %>% 
  mutate(log_troops_lag = lag(log_troops, order_by = year),
         troops_lag = lag(troops, order_by = year),
         anti_us_protest_lag = lag(anti_us_protest),
         troops_cumsumtest_unlagged= cumsum(coalesce(troops, 0)),
         troops_cumsumtest_lagged = lag(cumsum(coalesce(troops, 0)), length = 2),
         troops_cumsumtest = log1p(lag(cumsum(troops), length = 2)),
         troops_cumsum = log1p(lag(cumsum(coalesce(troops, 0)), length = 2))) %>% # Make sure to not drop Germany. This finds the first non-missing value in the vector and replacing the missing values with that. Should help bridge the gap with the missing values since the data start in the late 1980s.
  ungroup() %>% 
  mutate(log_troops_lag_z = arm::rescale(log_troops_lag),
         log_troops_z = arm::rescale(log_troops),
         log_troops_cumsum_z = arm::rescale(troops_cumsum)) %>% 
  group_by(year) %>% 
  mutate(troop.prop = troops/sum(troops, na.rm = TRUE))


# Make sure all countries are represented if they should be. Germany I'm looking at you!
ccodecheck <- p.data %>% 
  dplyr::select(ccode, countryname, year, troops, starts_with("troops_cumsum"), log_troops_lag_z, log_troops_z, log_troops_cumsum_z) %>% 
  filter(ccode == 255)

checkNA <- p.data %>% 
  filter(is.na(troops) & !is.na(lag(troops))) %>% 
  dplyr::select(ccode, year)
  
  
# Load opinion data for individual-level protest models.
load(here::here("../Book/Data/General/opinion.data.RData")) 

o.data <- o.data %>% 
  mutate(protest_dummy = ifelse(protest == "Never", 0,
                                ifelse(protest == "Don't know/Decline to answer", NA, 1)),
         protest_dummy = factor(protest_dummy, levels = c(0, 1), labels = c("No", "Yes")),
         log_basecount = log1p(basecount))


# Generate random binomial variable based on representative group variables and country.
# Use this to create a random training sample and then a test sample for the predictions.
prediction.data <- o.data %>% 
  group_by(country, age, gender, income.5.cat) %>% 
  mutate(sample.group = rbinom(n(), size = 1, prob = 0.7),
         sample.group = factor(sample.group, levels = c(0, 1), labels = c("Test", "Training")))

training.data <- prediction.data %>% 
  filter(sample.group == "Training")


# Note that we have to re-factor the levels for troops_crime_pers and protest_dummy.
# Because there are levels in the data that aren't in the model it thinks we're trying to force a new
# factor level into the test data. Stan models can only do this for population-level effects when those factors levels are estimated in the model itself.
test.data <- prediction.data %>% 
  ungroup() %>% 
  filter(sample.group == "Test") %>% 
  mutate(protest_dummy = factor(protest_dummy, levels = c("Yes", "No")),
         troops_crime_pers = factor(troops_crime_pers, levels = "Yes", "No"))



op.protest.1 <- readRDS(here::here("../Book/Output/Chapter-Protests/op.protest.1.rds"))
op.protest.2 <- readRDS(here::here("../Book/Output/Chapter-Protests/op.protest.2.rds"))
op.protest.3 <- readRDS(here::here("../Book/Output/Chapter-Protests/op.protest.3.rds"))
op.protest.4 <- readRDS(here::here("../Book/Output/Chapter-Protests/op.protest.4.rds"))

#op.protest.1$data$predicted <- as.data.frame(predict(op.protest.1))
#op.protest.2$data$predicted <- as.data.frame(predict(op.protest.2))
#op.protest.3$data$predicted <- as.data.frame(predict(op.protest.3))
#op.protest.4$data$predicted <- as.data.frame(predict(op.protest.4))


#op.predict.list <- list(op.protest.1$data, 
#                        op.protest.2$data, 
#                        op.protest.3$data, 
#                        op.protest.4$data)


# Set Seed
SEED <- 66502
set.seed(seed = SEED)

```

This chapter provides supplementary information on the book chapter dealing with individual protest behavior and anti-US protest activity at the aggregate level. Mirroring the main manuscript, we will first present supplementary information on the micro-level protest section and then present supplementary information on the macro-level protest section.


## Micro-Level Protest

The individual-level models consist of four separate models that build on one another. We Our focus here is to better understand the individual correlates of protest but also to understand the overall predictive performance of the models. More specifically, while we expect that the models will generally provide a better prediction with the addition of more predictor variables, we are interested in the relative gains from adding particular "blocks" of predictors. These blocks are described in the table below.

```{r individual-level-summary-table, echo=FALSE}

`Base Model` <- c("Base Count", "GDP", "Population", "Troops in country")
`Demographics` <- c("Age", "Education", "Ideology", "Gender", "Income", "Minority Status", "Base Count", "GDP", "Population", "Troops in country")
`Demographics and Attitudes` <- c("Age", "Education", "Ideology", "Gender", "Income", "Minority Status", "Attitude towards US military presence", "Assessment of US influence (Quantity)", "Assessment of US influence (Quality)", "Base Count", "GDP", "Population", "Troops in country")
`Demographics, Attitudes, and Experiences` <- c("Age", "Education", "Ideology", "Gender", "Income", "Minority Status", "Attitude towards US military presence", "Personal contact", "Network Contact", "Personal benefits", "Network benefits", "Personal experience with troops and crime", "Network experience with crime", "Assessment of US influence (Quantity)", "Assessment of US influence (Quality)", "Base Count", "GDP", "Population", "Troops in country")

list.length <- length(`Demographics, Attitudes, and Experiences`)
length(`Base Model`) <- list.length
length(`Demographics`) <- list.length
length(`Demographics and Attitudes`) <- list.length
length(`Demographics, Attitudes, and Experiences`) <- list.length

var.list <- as.data.frame(cbind(`Base Model`, 
                             `Demographics`, 
                             `Demographics and Attitudes`, 
                             `Demographics, Attitudes, and Experiences`)) %>% 
  mutate(across(everything(), ~ifelse(is.na(.x), "", .x)))



var.table <- kableExtra::kable(var.list, caption = "Predictive Model Specification for Individual Protest Activity \\label{tab:protestpredictionvariables}", booktabs = TRUE) %>% 
  kable_styling(latex_options = "striped", protect_latex = TRUE, font_size = 9) %>% 
  row_spec(0, bold = TRUE) %>% 
  column_spec(1, width = "3.0cm") %>% 
  column_spec(2, width = "3.0cm") %>% 
  column_spec(3, width = "3.0cm") %>% 
  column_spec(4, width = "3.0cm") %>% 
  footnote(number = c("Each model include varyings intercepts for country and year. All models also include the primary predictor variables as varying coefficients across the country grouping."),
           threeparttable = TRUE) 

var.table


```

In each of these models we estimate both population average or "fixed" effects, and also estimate varying effects for the primary predictors by country. This means that we are relaxing the assumption that there is a constant relationship between predictor variables and protest across countries.^[We add an additional layer of flexibility to the models by allowing correlation between all of the varying or ``random'' effects to vary as well. This relaxes the assumption that the correlation between these effects is 0 and allows the model to estimate each instead.] Table \@ref(tab:individual-level-summary-table) provides an overview of the specifications of these five models. The varying coefficients vary by country only and not by year. 

Our focus in using these models is to assess how good they are at predicting actual protest behavior. To do this, we divide our data into two samples: a training sample and a test sample This step helps to ensure that we are not overfitting our model to the particular data that we have collected. In other words, it ensures that the model is indeed a general one and not just one that works only with the data from the specific individuals we surveyed. This step also allows us to assess how well our model can predict protest when we test it out on "new" data that were not used to estimate the model (the "test sample" that we referred to earlier). 

Because we know some characteristics of the underlying populations, we group our opinion data according to the individual representative characteristics (age, income, gender, and respondent's country). We then randomly assign 80\% of the observations to the training data and the remaining 20\% to the test data. Given the relative infrequency of some response categories for questions like religious or gender self-identification, we opted to give more weight to the training data in order to give the model more information.  This ensures that both the training data and the test data are representative of these same variables. We use the training data to fit the models we present below. Once we fit the models, we use the test data to assess how accurate the models are at predicting protest.

In the main text we present a series of initial predictive diagnostics. When assessing the primary models we predict protest responses as being a "Yes" if the median predicted probability value is $\geq$ 0.50 and "No" if the median value is less than this threshold. This represents a fairly conservative assessment tool, but in this appendix we explore alternative prediction thresholds.

To put it differently, setting too high a barrier for inclusion in the "Yes" category is also problematic as it means we miss an increasingly large share of cases. There are practical considerations in devising such a threshold as well. For example, if we found that the true probability of protesting were to be a 10\% chance, then we would be wrong nine of ten times. However, our model provides information that these cases stand out from even lower-probability observations. 

Part of the decision calculus in choosing a threshold is judging the utility of having more false-positive or false-negatives and how that informs decision-making or policymaking. If a policymaker wants to guard against protests, then increasing the number of false positives to capture the true positives is worthwhile. However, if the event itself is non-consequential or involves a severe policy response, then we should up our threshold and accept a higher number of false negatives. For example, suppose individuals who did not protest were subject to intensive surveillance or harassment because authorities *believed* they were involved in protests. In that case, this sort of policy response could easily backfire and become a self-fulfilling prophecy that makes protest behavior *more likely*. 

Looking at Table \@ref(tab:percent-correct-targeted), we can see that lower classification thresholds generate relatively comparable figures for specificity and overall correct predictions across all models while yielding much better performance on the sensitivity metric. Using a lower probability threshold for determining which observations fall into the "Yes" category can thus improve overall predictive accuracy.


```{r percent-correct-targeted, echo=FALSE, eval=FALSE}

# Create loop to generate ROC curve for different p values
results <- list()

prob <-  1

for(prob in 1:1000) {
# Huzzah!
  
op.predict.list.2 <- lapply(op.predict.list, function(x) 
  dplyr::select(x, predicted, protest_dummy) %>% 
    mutate(predicted.value = ifelse(predicted$Estimate >= (prob/1000), 1, 0),
           match = ifelse(protest_dummy == "Yes" & predicted.value == 1 | protest_dummy == "No" & predicted.value == 0, 1, 0),
           percent.correct = sum(match)/length(match),
           false.neg = ifelse(predicted.value == 0 & protest_dummy == "Yes", 1, 0),
           false.pos = ifelse(predicted.value == 1 & protest_dummy == "No", 1, 0),
           true.neg = ifelse(protest_dummy == "No" & predicted.value == 0, 1, 0),
           true.pos = ifelse(protest_dummy == "Yes" & predicted.value == 1, 1, 0),
           total.no = ifelse(protest_dummy == "No", 1, 0),
           total.yes = ifelse(protest_dummy == "Yes", 1, 0)) %>% 
    dplyr::summarise(percent.correct = mean(percent.correct),
                     false.neg = sum(false.neg),
                     false.pos = sum(false.pos),
                     true.neg = sum(true.neg),
                     true.pos = sum(true.pos),
                     total.no = sum(total.no),
                     total.yes = sum(total.yes)) %>% 
    mutate(`False Positive Rate` = false.pos/total.no,
           `False Negative Rate` = false.neg/total.yes,
           `Sensitivity` = true.pos/total.yes,
           `Specificity` = true.neg/total.no,
           pvalue = prob/1000))

op.predict.df <- bind_rows(op.predict.list.2) %>% 
  mutate(Model = c("Base Model", "Demographics", "Demographics and Attitudes", "Demographics, Attitudes, and Experiences")) %>% 
  dplyr::select(Model, percent.correct, `False Positive Rate`, `False Negative Rate`, `Sensitivity`, `Specificity`, pvalue) 

results[[prob]] <- op.predict.df

prob <- prob + 1 
}

results.df <- bind_rows(results) %>% 
  mutate(Model = factor(Model, levels = c("Base Model", "Demographics", "Demographics and Attitudes", "Demographics, Attitudes, and Experiences")))



# Get rough estimate for point at which sensitivity and specificity are equal
pval <- summary(results.df$pvalue[round(results.df$Sensitivity, 2) == round(results.df$Specificity, 2) & results.df$Model=="Demographics, Attitudes, and Experiences"])[[3]]


results.df.targeted <- results.df %>% 
  filter(pvalue == round(pval, 2)) %>% 
  dplyr::select(Model, percent.correct, `False Positive Rate`, `False Negative Rate`, `Sensitivity`, `Specificity`) %>% 
  mutate_at(vars(2:6),
            ~ paste((round(., 3)*100), "%", sep = ""))

names(results.df.targeted) <- c("Model", "Correct", "False Positive", "False Negative", "Sensitivity", "Specificity")

op.predict.table.targeted <- kable(results.df.targeted, "latex", align = "lrrrrr", caption = "Predictive Model Performance with targeted probability threshold of 0.10.}", booktabs = TRUE) %>% 
  kable_styling(latex_options = "striped", font_size = 9) %>% 
  column_spec(1, width = "6.4cm") %>% 
  column_spec(2, width = "1.5cm") %>% 
  column_spec(3, width = "1.5cm") %>% 
  column_spec(4, width = "1.5cm") %>% 
  column_spec(5, width = "1.5cm") %>% 
  column_spec(6, width = "1.5cm") %>% 
  footnote(number = c("Correct predictions is the number of predicted values that match observed values divided by the total number of observations.",
                      "False positive rate is the number of incorrect positive predicted cases divided by the total number of actual observed negative cases. Rather, the percent of negative cases incorrectly classified as positive.", 
                      "False negative rate is the number of incorrect negative predicted cases divided by the total number of actual observed positive cases. Rather, the percent of positive cases incorrectly classified as negative.",
                      "Sensitivity is the number of predicted positive values divided by the total number of true positive values. Rather, the percent of positive cases that are correctly classified.",
                      "Specificity is the number of predicted negative values divided by the total number of true negative values. Rather, the percent of negative cases that are correctly classified."),
           fixed_small_size = TRUE,
           footnote_as_chunk = FALSE,
           threeparttable = TRUE,
           escape = FALSE) 


```


Table \@ref{tab:percent-correct-targeted} shows the models' predictive accuracy when we use 0.10 as the probability threshold for predicting individuals' protest experience. In general, the overall percent of observations correctly predicted remains fairly high but has declined slightly. We also see the false positive rate has increased, which we should expect from having a much lower threshold. More importantly, the sensitivity score has increased substantially across each model. The minimum here is approximately 41\%, with a high of 77\%. In other words, using a lower probability threshold, our final model is correctly classifying three-quarters of cases where individuals report having participated in anti-US protest events. Again, this increase does come with a cost. The lower overall rate of correct predictions results from the slight decrease in the correct predictions of ``No'' responses. The increased false-positive rate also reflects this. 


```{r protest-roc-plot, message=FALSE, echo=FALSE, fig.cap="Receiver operating characteristic curve plot for individual-level protest models."}

knitr::include_graphics(here::here("../Book/Figures/Chapter-Protests/fig-roc-plot.png"))

```


From the curves presented in Figure \@ref{fig:protest-roc-plot} we can see that there is indeed substantial variation in model performance when we account for the full range of probability thresholds. Ideally, we would want to see a model's curve spike up to 100\% immediately on the left side of the figure, but none of the models come close to this ideal type. The Intercept Only model performs the worst of the five, as we expect. Again, however, we see substantial improvements as we add demographic variables, and then demographic, attitudinal, and experiential variables. Ss this figure helps to make clearer, the relative performance of the three more complex models---including the one allowing for varying coefficient estimates---is nearly identical, with a very slight edge to the varying coefficient model. Ultimately, all of the models perform better than a simple random assignment mechanism. Still, some are clearly doing a better job of accurately classifying cases of protest involvement while sacrificing less in generating a larger share of false positives.


```{r fig-pvalue-comparison, message=FALSE, echo=FALSE, fig.cap="Figuring showing the percent predicted correctly, sensitivity, and specificity across different prediction categorization probability thresholds."}
knitr::include_graphics(here::here("../Book/Figures/Chapter-Protests/fig-pvalue-comparison.png"))
```


The disadvantage of the ROC plot is that it obscures the specific probability values and the trade-off between the different aspects of model performance that we seek to maximize. Figure \@ref(fig:protest-pvalue-compare) plots the sensitivity, specificity, and percent predicted correctly figures across the range of possible classification probability thresholds. In general, higher probability thresholds make it more difficult for a given observation to make it into the "Yes" category. Conversely, lower thresholds mean that we are more likely to classify observations as "Yes". This is a seemingly trivial point, but it is worth reiterating because there is nothing special about the 0.50 threshold we use above---it is simply a convention of sorts and seemingly "neutral". But as we can see from Figure \@ref(fig:protest-pvalue-compare) the trade-offs associated with moving between various probability thresholds are not all equal. In this case, our initial na\"{i}ve threshold of 0.50 sacrifices a considerable amount of performance on the sensitivity metric for relatively little gain on the specificity and overall correct prediction metrics. The percent of cases predicted correctly actually starts to decline slightly as we increase the probability threshold.

As one last check on our models' performance, Figure \@ref(fig:ppcheck-individual-protest) shows the results of a set of posterior predictive checks using the five protest models discussed in this section. We generate 1,000 simulated data sets for each model, each containing simulated predicted Yes/No values for the outcome variable. For each data, set we take the mean value of these predictions, giving us the proportion of each simulated data set where the model predicts individuals to have responded "Yes" to the outcome variable question. The light blue histograms in each panel show the distribution of these predicted proportion values. The dark line shows the actual proportion of "Yes" responses in the real survey data. The narrow bands on the X-axis somewhat mask the fact that the actual distribution of simulated values is fairly narrow in every case. All five models produce reasonably close approximations of the real data, though the mean values are slightly below the actual mean value in every panel except the Intercept Only model. The model including only respondents' demographic characteristics produces the closest set of simulated values, though with slightly greater dispersion than the other more complex and fully specified models. 






## Macro-Level Protest

In the book's main text we show a series of models that examine the predictors of anti-US and anti-US military protest events for a sample of country-years ranging from 1990--2018. We are fundamentally interested in exploring whether the presence of US military forces in another country *causes* an increase in protest activity directed at the United States, or more narrowly, at the US military forces in the host country. While this may seem obvious, the identification of actual causal effects is a fairly complicated task. The presence of US military forces in a state may coincide with several other types of events that could explain both the US military presence and the outbreak of protests. Furthermore, the outbreak of protests at any given point in time may be driven by an accumulation of historic factors that pre-date the observed time period. Our purpose in this section is to elaborate upon the analytical tools and the process we use to deal with these issues.

While the first set of macro-level protest models we present in the book provide a useful starting point in exploring the relationship between the presence of US military deployments and protest activity, we cannot make causal claims on the basis of these results. Basic regression models typically only provide estimates of short-term effects. Further, while the US has a expansive military footprint, the histories of these deployments are highly variable across countries. This can severely bias the estimates from our model as not every country has an equal opportunity of experiencing US military deployments—both in terms of hosting them and the size of those deployments. Finally, as is often the case, the relationship between military deployments and protest events may suffer from confounding effects. Other variables may exert a causal effect on both of our variables of interest. Accordingly, modeling protest events as a function of military deployments and adjusting for many other variables will not yield results that have any meaningful interpretation as causal effects. 

To address these problems, we estimate a marginal structural model (MSM) to estimate the causal effects of US military deployments on protest [@Robinsetal2000; @BlackwellGlynn2018]. These models have a relatively long history of use in fields like biostatistics and epidemiology and can be used to help estimate causal effects in observational studies where exposure to treatments varies across space and time. We can also use this to help us estimate the contemporaneous effect of treatment and the more general effect of a particular treatment history. However, MSMs require us to do some additional work before estimating the effects of troop deployments and their histories on the outcome of interest. There will likely be systematic differences between the individual countries that receive troop deployments or larger deployments and those that receive no or smaller deployments.

```{r, protest-dag,  echo = FALSE, dpi=400, warning=FALSE, message=FALSE, fig.cap="Simplified Directed Acyclic Graph (DAG) illustrating the theoretical relationships between the treatment, confounders, and outcome variables."}

library(tidygraph)
library(latex2exp)
library(dagitty)
library(ggraph)

# Draw DAG for Protest Count

coords <- list(x = c(Protest = 6, Troops = 5, Growth = 1, GDP = 0, Trade = 1, Population = 0, Regime = 3, Conflict = 1, Ally = 3),
               y = c(Protest = 6, Troops = 4, Growth = 4, GDP = 5, Trade = 2, Population = 7, Regime = 8, Conflict = 8, Ally = 4))

daglabels <- c(Protest = "Protest", Troops = "Troops[t-1]", Growth = "Growth", GDP = "GDP", Trade = "Trade", Population = "Population BIG", Regime = "Regime Type", Conflict = "War", Ally = "Alliance")

dagbase <- dagify(Protest ~ Troops + Growth + Population + Regime + Conflict,
                  Troops ~ Regime + Conflict + Ally + Trade,
                  Ally ~ Trade + GDP + Regime,
                  Trade ~ Ally + GDP + Regime,
                  GDP ~ Trade + Population + Conflict,
                  Growth ~ Conflict + Trade + GDP,
                  Regime ~ Conflict,
                  exposure = "Troops",
                  outcome = "Protest",
                  coords = coords,
                  labels = daglabels) %>% 
  tidy_dagitty() 
 
#ggdag(dagbase)

dagbase2 <- dagitty(' dag{
                    "Troops[t]" [exposure]
                    "Protest[t]" [outcome]
                    
                    "Protest[t-1]" [pos="1,0"]
                    "Troops[t-1]" [pos = "0,-0.5"]
                    "Threat[t-1]" [pos="-1.1,1.5"]
                    "Growth[t-1]" [pos="-1,3"]
                    "Population[t-1]" [pos="-1.25,0"]
                    "Regime[t-1]" [pos="-1.25,-1.5"]
                    "GDP[t-1]" [pos="-1.1,-3"]
                    "Rebellion[t-1]" [pos="-0.75,-6"]
                    "Trade[t-1]" [pos="-0.5,-7"]
                    "Ally[t-1]" [pos="-1,-4.5"]
                    "Alignment[t-1]" [pos="-0.75,4.5"]
                    "ProtestEnvironment[t-1]" [pos="0,6"]
                    "USWar[t-1]" [pos="0,-8"]

                    "Protest[t]" [pos="5,0"]
                    "Troops[t]" [pos = "4,-0.5"]
                    "Threat[t]" [pos="2.9,1.5"]
                    "Growth[t]" [pos="3,3"]
                    "Population[t]" [pos="2.75,0"]
                    "Regime[t]" [pos="2.75,-1.5"]
                    "GDP[t]" [pos="2.9,-3"]
                    "Rebellion[t]" [pos="3.25,-6"]
                    "Trade[t]" [pos="3.5,-7"]
                    "Ally[t]" [pos="3,-4.5"]
                    "Alignment[t]" [pos="3.25,4.5"]
                    "ProtestEnvironment[t]" [pos="4,6"]
                    "USWar[t]" [pos="4,-8"]
                    
                    "Protest[t-1]" -> "Protest[t]"
                    "ProtestEnvironment[t-1]" -> "ProtestEnvironment[t]"
                    "Troops[t-1]" -> "Troops[t]"
                    "Population[t-1]" -> "Population[t]"
                    "Regime[t-1]" -> "Regime[t]"
                    "Threat[t-1]" -> "Threat[t]"
                    "GDP[t-1]" -> "GDP[t]"
                    "Growth[t-1]" -> "Growth[t]"
                    "Rebellion[t-1]" -> "Rebellion[t]"
                    "Trade[t-1]" -> "Trade[t]"
                    "Ally[t-1]" -> "Ally[t]"
                    "Alignment[t-1]" -> "Alignment[t]"
                    "ProtestEnvironment[t-1]" -> "ProtestEnvironment[t]"
                    "USWar[t-1]" -> "USWar[t]"
                    
                    
                    "Regime[t]" -> {"Troops[t]" "ProtestEnvironment[t]" "Trade[t]" "Rebellion[t]" "Alignment[t]" "USWar[t]"}
                    "Threat[t]" ->  {"Troops[t]" "Alignment[t]"}
                    "Population[t]" -> "ProtestEnvironment[t]"
                    "Population[t]" -> {"GDP[t]" "Trade[t]"}
                    "Troops[t]" -> {"Protest[t]" "Growth[t]"}
                    "Growth[t]" -> {"ProtestEnvironment[t]" "Protest[t]" "Rebellion[t]"}
                    "Ally[t]" -> {"Troops[t]" "Protest[t]" "Trade[t]" "USWar[t]"}
                    "Trade[t]" -> {"GDP[t]"}
                    "GDP[t]" -> {"Trade[t]" "Rebellion[t]"}
                    "Alignment[t]" -> {"Ally[t]" "USWar[t]" "Protest[t]"}
                    "ProtestEnvironment[t]" -> {"Rebellion[t]" "Protest[t]"}
                    "USWar[t]" -> {"Protest[t]" "Protest[t]"}
                    "Rebellion[t]" -> {"Protest[t]"}


                    "Regime[t-1]" -> {"Troops[t-1]" "ProtestEnvironment[t-1]" "Trade[t-1]" "Rebellion[t-1]" "Alignment[t-1]" "USWar[t-1]"}
                    "Threat[t-1]" ->  {"Troops[t-1]" "Alignment[t-1]"}
                    "Population[t-1]" -> "ProtestEnvironment[t-1]"
                    "Population[t-1]" -> {"GDP[t-1]" "Trade[t-1]"}
                    "Troops[t-1]" -> {"Protest[t-1]" "Growth[t-1]"}
                    "Growth[t-1]" -> {"ProtestEnvironment[t-1]" "Protest[t-1]" "Rebellion[t-1]"}
                    "Ally[t-1]" -> {"Troops[t-1]" "Protest[t-1]" "Trade[t-1]" "USWar[t-1]"}
                    "Trade[t-1]" -> {"GDP[t-1]"}
                    "GDP[t-1]" -> {"Trade[t-1]" "Rebellion[t-1]"}
                    "Alignment[t-1]" -> {"Ally[t-1]" "USWar[t-1]" "Protest[t-1]"}
                    "ProtestEnvironment[t-1]" -> {"Rebellion[t-1]" "Protest[t-1]"}
                    "USWar[t-1]" -> {"Protest[t-1]" "Protest[t-1]"}
                    "Rebellion[t-1]" -> {"Protest[t-1]"}


                    "Troops[t-1]" -> "Protest[t]"
                    "USWar[t-1]" -> "Troops[t]"

                    }') 

dagbasetidy <- tidy_dagitty(dagbase2)

dagbase2 %>% 
  tidy_dagitty() %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_dag_edges_arc(curvature = 0.05) +
    geom_dag_text(parse = TRUE, color = "black", size = 4) +
    theme_dag_blank()


#adjustmentSets(dagbase2)

#dagbase2

```

Figure \@ref(fig:protest-dag) presents a directed acyclic graph (DAG) depicting the basic contemporaneous relationships between the variables of interest and their relationships between two time periods, $t$ and $t+1$ (which is the period directly following t). This figure is a bit of a mess, visually speaking, and it can be difficult to navigate the tangle of arrows within time periods. Much of this complexity is also unnecessary to understand the basic intuition that we want to convey, so we present Figure \@ref(fig:protest-dag-simple) to distill these relationships down to some of the most basic and relevant parts. 

We condense most of the time-variant and time-invariant predictor variables down to the time-invariant covariates ($Z$ terms). We also only present two periods, but the chain of events theoretically runs back to the first observation of $t$ in the series. The light blue node represents the outcome of interest (protests), while the light green node represents the treatment of interest (troops). Any node connected to another indicates a proposed causal relationship, and the direction of the arrow represents the direction of that causal effect Additionally, troop deployments in a given period (what we refer to as time $t$) are heavily dependent on deployments in the previous period (time $t-1$). Deployments at time $t-1$ are also likely to affect other predictor variables in subsequent periods. 


```{r protest-dag-simple, echo=FALSE, warning=FALSE, fig.cap="Simplified Directed Acyclic Graph (DAG) illustrating the theoretical relationships between the treatment, confounders, and outcome variables."}
# Prettier DAG for public consumption in text

dag.pretty <- dagitty(' dag{
                    "Troops[t]" [exposure]
                    "Protest[t]" [outcome]
                    
                    "Protest[t]" [pos="1,1"]
                    "Troops[t]" [pos = "0,0"]
                    "Z[t]" [pos="0,2"]
                    
                    "Protest[t-1]" [pos="-1,1"]
                    "Troops[t-1]" [pos = "-2,0"]
                    "Z[t-1]" [pos="-2,2"]

                    "Protest[t-1]" -> "Protest[t]"
                    "Troops[t]" -> "Protest[t]"
                    "Troops[t-1]" -> "Troops[t]"
                    "Troops[t-1]" -> "Protest[t-1]"
                    "Z[t-1]" -> "Z[t]"
                    "Z[t]" -> "Troops[t]"
                    
                    "Troops[t-1]" -> "Protest[t]"
                    "Z[t-1]" -> "Troops[t-1]"
                    "Z[t-1]" -> "Protest[t-1]"
                    "Z[t]" -> "Protest[t]"

                    }') 


dag.pretty.tidy <- tidy_dagitty(dag.pretty) %>% 
  mutate(.$data, 
         vartype = case_when(
           name == "Troops[t]" ~ "Exposure",
           name == "Protest[t]" ~ "Outcome",
           grepl("Z", name) ~ "Confounders",
           name == "Troops[t-1]" ~ "Lagged Treatment",
           name == "Protest[t-1]" ~ "Lagged Outcome"

         )) %>% 
  dag_label(labels = c("Troops[t]" = "Troops[t]", "Troops[t-1]" = "Troops[t-1]", "Protest[t-1]" = "Protest[t-1]",
                       "Protest[t]" = "Protest[t]", "Z[t]" = "Z[t]", "Z[t-1]" = "Z[t-1]")) 

 
chart <- ggplot(dag.pretty.tidy, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(aes(color = vartype)) +
  geom_dag_edges() +
  geom_dag_label_repel(aes(label = label), parse = TRUE) +
  theme_dag_blank() +
  guides(color = FALSE) +
  scale_color_manual(values = c("orange",  "#009E73", muted("dodgerblue1"), muted("009E73"), "dodgerblue1"))

chart

```

These diagrams serve as the starting point for our marginal structural models. Before we can estimate the models we need to ensure that we have a solid theoretical foundation to inform the model's specification. We will return to discuss the DAGs further below. Now we will discuss the specific procedure for estimating the marginal structural models.

The first step in the model building process is to estimate a series of structural weights for each observation for the final regression model to resolve this. We calculate these weights using the formula found Equation \@ref{eq:structuralweights}.


\begin{equation}
\hat{SW}_{it}=\prod_{t=1}^{N}\frac{ { {\hat{Pr}(X_{it}|X_{it-1},\displaystyle\sum_{k=2}^{N}x_{t-k},Z_{i})} } } {\hat{Pr}(X_{it}|X_{it-1},\displaystyle\sum_{k=2}^{N}x_{t-k},\gamma_{it},Z_{i})}
\end{equation}

More simply, the weights shown in Equation \@ref(eq:structuralweights) are a form of propensity score that we can use to re-balance the observations in our data. This is simply a variant of the inverse probability of treatment weighting (IPTW) method.^[For more information on estimating propensity scores see: @ImbensRubin2015 Chapter 13] Normally, estimating propensity scores is a relatively straightforward process as it is commonly used to estimate scores in data where the treatment, and often the outcome, are binary (meaning 0 or 1) variables. The fact that our treatment (the number of US troops present in a country) is continuous, meaning that it can, in theory, take on any integer value that is 0 or greater, complicates this process slightly.^[For a fuller discussion of estimating the inverse probability of treatment weights for MSMs, estimating these weights for continuous treatment variables, or estimating weights for a continuous treatment in the presence of multilevel/grouped data using multilevel models see the following works: @ColeHernan2008;@vanderwalGeskus2011;@Naimietal2014;@SchulerChuCoffman2016]

To generate these weights we first have to estimate two separate models wherein troop deployments themselves are the outcome of interest. The first of these models (the numerator) is relatively straightforward to estimate. Here we are running a regression model wherein we predict the size of the troop deployments in country $i$ at time $t$ as a function of a one-unit lag of the troop deployment variable, the cumulative sum of troop deployments in the country $i$ from the beginning of its series up through time $t-2$, and a range of time-invariant covariates, $Z$. In cases where researchers are working with just a binary treatment variable and only a single period, using "1" as the numerator is often all that is required. However, since we are dealing with a continuous treatment variable, generating the numerator using this regression-based approach can help to stabilize the resulting weights by ensuring that the ratios are not enormous [@ColeHernan2008].

The second model (the denominator) is more complicated. In most respects, it is identical to the numerator, except the gamma ($\gamma$) term, which denotes a vector of covariates---the key here is that these covariates have to be sufficient to meet the sequential ignorability criteria. In essence, this requires that there is no unmeasured confounding between the treatment (troops) and the outcome (protests).

The language on this point can be confusing given that these techniques were developed across several different literatures and disciplines and compounded by the fact that they often did not speak to one another. To put it in slightly different terms using language from Pearl, this set of covariates must ensure that Pr$(Y_{it} \perp X_{it} | \gamma, Z_{i})$. That is, the outcome is conditionally independent of the treatment, conditional upon the time-varying covariates ($\gamma$) and the time-invariant covariates ($Z$) [@Pearl2009].

We will return to this below, but once we have both of these models, we then generate two sets of predicted values and residuals for each observation in the data set---one using the numerator and one using the denominator. For each set of predictions and residuals, we calculate the probability of the actual observed value of troops in country $i$ at time $t$ using the predictions and the standard deviation of the residuals as the mean and standard deviation of a normal probability density function. We then divide the values generated using the numerator figures by the values generated by the numerator. The actual structural weight for any given observation is the cumulative product of these ratios for country $i$ from the beginning of that country's series up through time $t$. 

But, as we note above, estimating the second model is complicated because we need a set of covariates that ensure there is no unmeasured confounding between the treatment and outcome variables. To aid in this process, we turn to another tool---the directed acyclic graphs (DAGs) discussed above. Like with MSMs, scholars from other disciplines have more commonly used DAGs (such as computer science and epidemiology), and they serve a variety of useful purposes. First, they help make explicit the wider array of theorized causal relationships between various predictor variables. Second, assuming a well-developed theoretical model, they can also help to identify sources of confounding and bias.^[Keele et al. offer a fuller discussion of DAGs and their applications. @Pearl2009;@MorganWinship2015;@KeeleStevensonElwert2020] Related, they can be used to identify which variables need to be adjusted for in a statistical model to close off the ``back-door'' paths between the treatment variable and the outcome that serve as the sources of confounding in the model, but also to ensure that we do not adjust for variables that may open up such informational pathways between the treatment and outcome and introduce additional sources of bias (i.e. collider bias) [@Pearl2009;@Textoretal2016].^[We use the dagitty package in to build our DAG and to identify possible adjustment sets.]

Using this model as our starting point, we can generate several adjustment sets---variables that will ensure there is no unmeasured confounding when included in the model. Notably, this rests entirely on the notion that we have the "right" theoretical model. We can use many possible adjustment sets given the current model, but they all accomplish the same basic task. If we have included causal relationships in our theoretical model that do not exist, or if we have omitted key causal relationships that *do* exist, then the implications of the model and our ability to isolate causal effects may change. How these changes would affect our ability to assess the causal relationships we are proposing depends entirely on the specific connections that would change. Assuming for present purposes that our model is sufficient, we can now estimate the denominator described in Equation \@ref(eq:structuralweights).

Once we have calculated the structural weights using this method, we can now estimate the average treatment effect (ATE) of troop deployments on protest activity. To do so, we estimate a series of multilevel negative binomial regressions predicting the number of protests in a country year using the troop deployment variable and treatment history (i.e. the cumulative sum of deployments through time $t-2$) as predictor variables.^[For more information on the use of multilevel models for causal inference see: @Hill2013]  Notably, these models weight the observations according to the structural weights we calculated above. US military deployments are generally stable *within* countries, with substantial variation *between* countries. For example, Germany's troop levels are high and relatively stable over the period that we study. In other cases, troop levels are low but also relatively stable. There are, however, some cases where troop levels increase and decrease radically at various points throughout 1990--2018. These sudden changes are typically associated with large military operations, like the 1999 war in Kosovo or the 2003 Iraq War. This extreme variability makes estimating the outcome models somewhat difficult because we have extremely large weights for a small set of observations. To address this problem, we estimate the models multiple times using several different truncation points for the weights to better assess the sensitivity of our estimates to the weighted sample. The weights effectively create a pseudo sample of data by replicating observations according to their score. For example, an observation with a score of 4 would be copied four times when we run the model. Given these cases where we see very rapid and extreme changes in US military deployment levels, we encounter a situation where the weights are often enormous (such as weighting scores of 50,000$+$). There is some question about the proper size of the weights and how to deal with very large or very small weights. Because we are dealing with some exceptionally large weights, we estimate our outcome model multiple times using different truncation points to cut down on the number of extreme values and to assess how sensitive our estimates of the ATE are to changes in the weights.^[There is no clearly "correct" way to deal with extreme weight values. Cole and Hernan note that in the context of binary treatment variables, the average of the weights should be approximately 1. Still, beyond this, there is relatively little guidance of which we are aware [@ColeHernan2008]. One approach is to trim weight scores, which we employ here. Specifically, we set multiple thresholds at 10, 50, 500, 1,000, 5,000, and 10,000. Observations, where the calculated weight falls above the threshold reset to the maximum threshold. For example, when estimating our models using the 500 threshold, a structural weight calculated to be 1,000 would be re-coded to be 500. This approach is useful in that it does not require us to throw away data. There is a clear gain in efficiency due to increasing the effective number of observations, but too few or too many pseudo observations can bias the estimates. Hence, we present all of these points to assess bias and sensitivity.]

Our approach to estimating these models is not perfect, and there are a few issues that readers should make note of. First, we face a tradeoff between estimating multiple models using different IPTW truncation points and estimating a "fully" Bayesian marginal structural model. The latter would involve estimating the outcome model several times to incorporate the uncertainty associated with the propensity scores and weights. We initially attempted to reconcile the two approaches by using the `brms_multiple()` command to estimate the model several hundred times using a mixture of different truncation points. However, we think this approach is less than ideal because it fails to address the uncertainty arising from the distribution in predicted weights, and treats fundamentally different "datasets" resulting from the truncation caps as interchangeable. 

Second, one drawback of the large pseudo samples is that the credible intervals around the point estimates are likely incorrect. The pseudo sample literally copies observations in the data set, increasing the overall sample size. This has the effect of reducing the size of the intervals surrounding the point estimates. 

Andrew Heiss and Jordan Nafa have a [working paper](https://www.andrewheiss.com/research/working-papers/heiss-nafa-bayes-ipw/) that tries to resolve some of these issues. I'd encourage readers to check it out. 



